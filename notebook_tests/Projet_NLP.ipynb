{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import des bibliothèques nécessaires**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine learning : Import du fichier généré pour effectuer les tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/nlp_generated_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texte</th>\n",
       "      <th>sujet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Je suis blessé, j’ai besoin d’un centre médica...</td>\n",
       "      <td>santé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Je cherche un centre de santé à Paris où je po...</td>\n",
       "      <td>santé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>J’ai des pensées suicidaires, je suis à avenue...</td>\n",
       "      <td>santé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y a-t-il un hôpital ou un médecin proche de ru...</td>\n",
       "      <td>santé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Je me suis blessé et je saigne. Où aller à Par...</td>\n",
       "      <td>santé</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texte  sujet\n",
       "0  Je suis blessé, j’ai besoin d’un centre médica...  santé\n",
       "1  Je cherche un centre de santé à Paris où je po...  santé\n",
       "2  J’ai des pensées suicidaires, je suis à avenue...  santé\n",
       "3  Y a-t-il un hôpital ou un médecin proche de ru...  santé\n",
       "4  Je me suis blessé et je saigne. Où aller à Par...  santé"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine learning : Nettoyage du fichier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   texte   2000 non-null   object\n",
      " 1   sujet   2000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 31.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [texte, sujet]\n",
      "Index: []\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   texte   2000 non-null   object\n",
      " 1   sujet   2000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 31.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# On vérifie si la colonne contient des doublons et on les traite s'il y en a.\n",
    "\n",
    "filtered_df = df[df['texte'].isna()]\n",
    "print(filtered_df)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "df.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texte\n",
      "<class 'str'>    2000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# On vérifie qu'on a bien que des valeurs avec un type String\n",
    "\n",
    "print(df['texte'].apply(type).value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réaliser le / les modèles de machine learning\n",
    "# Le but : identifier le sujet concerné par le texte envoyé par l'utilisateur.\n",
    "# Ce sujet identifié nous permettra de cibler la bonne table à requêter + nous servira aussi dans la partie NLP pour nommé ce sujet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**API Chatbot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLP : Chargement du modèle français de spaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")  # Chargement du modèle français\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLP : Réception d'un texte en entrée**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vous :  Je cherche les horaires d’une maraude dans Paris, si possible près de rue de Robin.\n"
     ]
    }
   ],
   "source": [
    "# Etant dans un notebook Jupyter, je ne peux pas utiliser un input() pour le moment. Il faudra attendre de transférer le code dans un fichier python .py\n",
    "# received_text = input(Bonjour ! Que puis-je faire pour vous ?\")\n",
    "\n",
    "received_text = \"Je cherche les horaires d’une maraude dans Paris, si possible près de rue de Robin.\"\n",
    "print (\"vous : \", received_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1996645456.py, line 22)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mruler.add_patterns(patterns)\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Je veux que \"Rue de Robin\" soit reconnu comme une localisation. Actuellement \"Robin\" est détecté comme étant une personne.\n",
    "# On utilise donc le composant de spaCy nommé EntityRuler por spécifier et prioriser des règles d'entités.\n",
    "from fastapi import FastAPI\n",
    "from spacy.pipeline import EntityRuler\n",
    "app = FastAPI()\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    " # On crée les patterns à identifier comme étant une localisation. \n",
    " \n",
    "patterns = [\n",
    "   \n",
    "        {\n",
    "            \"label\": \"LOC\",\n",
    "            \"pattern\": [\n",
    "                {\"LOWER\": {\"IN\": [\"rue\", \"avenue\", \"boulevard\", \"place\"]}},\n",
    "                {\"LOWER\": {\"IN\": [\"de\", \"d'\", \"du\", \"des\"]}},\n",
    "                {\"IS_ALPHA\": True, \"OP\": \"+\"}  # pour détecter un ou plusieurs mots alphabétiques\n",
    "            ]\n",
    "    },\n",
    "        {\n",
    "        \"label\": \"LOC\",  # Identifier comme une localisation\n",
    "        \"pattern\": [{\"SHAPE\": \"ddddd\"}]  # Reconnaître les séquences de 5 chiffres\n",
    "    }\n",
    " ]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "\n",
    "\n",
    "@app.post(\"/analyze/\")\n",
    "def analyze_text(text: str):\n",
    " \n",
    " spacy_doc = nlp(received_text)\n",
    "\n",
    " for token in spacy_doc : \n",
    "    print(f\"{token.text.lower()} → lemma: {token.lemma_.lower()}, POS: {token.pos_}\")\n",
    "    \n",
    " for ent in spacy_doc.ents:\n",
    "    print(f\"{ent.text} → label: {ent.label_}\")\n",
    "\n",
    " lieux = [ent.text for ent in spacy_doc.ents if ent.label_ in [\"GPE\", \"LOC\", \"FAC\"]]\n",
    " print(\"LIEUX DÉTECTÉS :\", lieux)\n",
    "\n",
    " clean_text = \" \".join([\n",
    "    token.lemma_.lower() for token in spacy_doc\n",
    "    if not token.is_stop and not token.is_punct\n",
    "])\n",
    " print(\"Texte nettoyé :\", clean_text)\n",
    "\n",
    " # En attendant d'avoir réalisé l'algorithme de machine learning permettant de classer le texte de l'utilisateur comme étant \"santé\", \n",
    " # \"judiciaire\", \"maraude\" ou \"douche\", on simule un sujet = \"santé\" pour la suite du text.\n",
    " sujet = \"santé\"\n",
    " # sujet = classifier_subject_model.predict(text)\n",
    "\n",
    "\n",
    " if len(lieux) == 1:\n",
    "    print(f\"Vous vous intéressé au sujet {sujet} sur la localisation {lieux[0]}\")\n",
    " elif len(lieux) == 2:\n",
    "    print(f\"Vous vous intéressé au sujet {sujet} sur la localisation {lieux[0]}, {lieux[1]}\")\n",
    " elif len(lieux) == 3:\n",
    "    print(f\"Vous vous intéressé au sujet {sujet} sur la localisation {lieux[0]}, {lieux[1]}, {lieux[2]}\")\n",
    " elif len(lieux) == 4:\n",
    "    print(f\"Vous vous intéressé au sujet {sujet} sur la localisation {lieux[0]}, {lieux[1]}, {lieux[2]}, {lieux[3]}\")\n",
    " else :\n",
    "    print(f\"Vous vous intéressé au sujet {sujet}. Pouvez-vous nous donner le code postal, s'il vous plait ?\")\n",
    "\n",
    "\n",
    " return classifier(text)\n",
    "\n",
    "\n",
    "# Lancer avec : uvicorn script:app --reload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si pas de code postal dans \"lieux\" alors le demander\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour la suite, je pensais trouver un moyen de matcher la / les localisations trouvées avec un code postal.\n",
    "# Ce code postal pourrait nous permettre de proposer les hopitaux, commissariats, maraudes, douches (en fonction du sujet concerné) \n",
    "# dans le code postal (via requête BDD / table concernée)\n",
    "\n",
    "# Si pas de localité ou localité insuffisante pour matcher un code postal, on pourrait proposer les deux premiers chiffres des 8 départements de L'ile de france,\n",
    "# en fonction du choix réalisé, on pourrait proposer la list de CP, et en fonction du choix, la liste des hopitaux, commissariats, \n",
    "# maraudes, douches (en fonction du sujet concerné).\n",
    "\n",
    "# Pour la liste des des CP et informations , on pourrait scrapper des sites (tel que : https://www.hopital.fr/annuaire/Paris+Ile-de-France+75/) \n",
    "# ou réaliser un fichier nous mêmes avec de fausses informations pour simuler les données ou utiliser une API disponible pour récupérer ces infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pas sûre de garder cette partie sur TfidfVectorizer. On a trop de lignes et trop de caractères par ligne pour pouvoir voir quelque chose j pense\n",
    "\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#vectorizer = TfidfVectorizer()\n",
    "#X = vectorizer.fit_transform(df[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vectorizer.get_feature_names_out())  # Liste des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X.toarray()[10:30])  # Matrice TF-IDF sur un échantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nfrom transformers import pipeline\\nclassifier = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\\n\\nresult = classifier(\"J\\'adore ce produit, il est excellent !\")\\nprint(result)\\n\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    " \n",
    "result = classifier(\"J'adore ce produit, il est excellent !\")\n",
    "print(result)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
